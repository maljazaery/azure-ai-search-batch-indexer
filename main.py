# Azure AI Document Intelligence  - Batch Processing
# This code demonstrates an example of using [LangChain](https://www.langchain.com/) to delvelop AI based PDF file processer and chunker using parallel processing with multi endpoints. It uses Azure AI Document Intelligence as document loader, which can extracts tables, paragraphs, and layout information from pdf, image, office and html files. The output markdown can be used in LangChain's markdown header splitter, which enables semantic chunking of the documents. 

# ## Prerequisites


from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader
from langchain.text_splitter import MarkdownHeaderTextSplitter
import concurrent.futures
import os
import random
import yaml
import argparse

# Create the parser
parser = argparse.ArgumentParser(description="Process some files.")

# Add the arguments
parser.add_argument('input_dir', type=str, help="The directory containing the input files.")
parser.add_argument('output_dir', type=str, help="The directory to write the output files.")
parser.add_argument('config_file', type=str, help="The config file.")

# Parse the arguments
args = parser.parse_args()

# Get the directories from the arguments
input_dir = args.input_dir
output_dir = args.output_dir
config_file = args.config_file


if not os.path.exists(output_dir):
    # If not, create it
    os.makedirs(output_dir)



# Load configuration from YAML file
with open(config_file, 'r') as stream:
    config = yaml.safe_load(stream)



# Function to save the parsed text to a file
def save_parsed_text(file_path, doc_string, output_dir=output_dir, input_dir=input_dir):
    # Get the base name of the file
    ext = "." + file_path.split(".")[-1]
    base_name = file_path.replace(input_dir, "").replace(ext, ".txt")

    # Create the output file path
    output_file = os.path.join(output_dir, base_name)

    # Write the doc_string to the output file
    with open(output_file, 'w') as f:
        f.write(doc_string)
    print(f"File saved to {output_file}")


# Split the document into chunks base on header markdowns which are generated by Azure AI Document Intelligence .
def split_text(doc_string):
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
    ]
    text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)

    splits = text_splitter.split_text(doc_string)
    return splits

# Function to process the document
def load_and_split(file_path):
    print(f"print processing {file_path}")
    
    # Choose a random endpoint-key pair
    endpoints_keys = config['endpoints_keys']
    endpoint_key = random.choice(endpoints_keys)
    endpoint = endpoint_key['endpoint']
    key = endpoint_key['key']
    print(f"Using endpoint: {endpoint}")
    
    loader = AzureAIDocumentIntelligenceLoader(file_path=file_path, api_key = key, api_endpoint = endpoint, api_model="prebuilt-layout")
    doc = loader.load()
    doc_string = doc[0].page_content
    
    return file_path,doc_string





# List of files to process
files = [os.path.join(input_dir, file) for file in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, file))]

# Use ThreadPoolExecutor to process the files in parallel
with concurrent.futures.ThreadPoolExecutor() as executor:
    # Use list comprehension to get a list of futures
    futures = [executor.submit(load_and_split, file) for file in files]

    for future in concurrent.futures.as_completed(futures):
        file_path = future.result()[0]
        output_txt = future.result()[1]
        print(f"Done: {file_path}")
        # Save the parsed text to a file
        save_parsed_text(file_path, output_txt)
        # Split the output into chunks
        splits = split_text(output_txt)
        print(f"Number of splits: {len(splits)}")

        #add logic to process the splits 
        #...
        #...
       

